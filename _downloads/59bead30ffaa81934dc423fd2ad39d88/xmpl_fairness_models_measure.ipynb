{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Measuring Fairness of a Predictive Model -- Disparate Impact\n\n\nThis example illustrates how to measure Disparate Impact of a predictive model.\nIn this example we measure the three most common Disparate Impact measures:\n\n* Equal Accuracy;\n* Equal Opportunity; and\n* Demographic Parity.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Our implementation of the k-nearest neighbours model\n   (:class:`fatf.utils.models.models.KNN`) works with structured numpy arrays,\n   therefore we do not have to pre-process (e.g., one-hot encode) the\n   categorical (stirng-based) features.\n\n    For scikit-learn models all of the categorical features in the data set\n    would need to be pre-processed first.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Kacper Sokol <k.sokol@bristol.ac.uk>\n# License: new BSD\n\nimport fatf.utils.data.datasets as fatf_datasets\nimport fatf.utils.models as fatf_models\n\nimport fatf.fairness.models.measures as fatf_mfm\n\nimport fatf.utils.metrics.tools as fatf_mt\n\nprint(__doc__)\n\n# Load data\nhr_data_dict = fatf_datasets.load_health_records()\nhr_X = hr_data_dict['data']\nhr_y = hr_data_dict['target']\nhr_feature_names = hr_data_dict['feature_names']\nhr_class_names = hr_data_dict['target_names']\n\n# Train a model\nclf = fatf_models.KNN()\nclf.fit(hr_X, hr_y)\n\n# Get predictions of the model for the fairness evaluation (which is also the\n# training data in this example)\nhr_pred = clf.predict(hr_X)\n\n# Select a protected feature\nprotected_feature = 'gender'\n\n# Get a confusion matrix for all sub-groups according to the split feature\nconfusion_matrix_per_bin, bin_names = fatf_mt.confusion_matrix_per_subgroup(\n    hr_X, hr_y, hr_pred, protected_feature, treat_as_categorical=True)\n\n\ndef print_fairness(metric_name, metric_matrix):\n    \"\"\"Prints out which sub-populations violate a group fairness metric.\"\"\"\n    print('The *{}* group-based fairness metric for *{}* feature split '\n          'are:'.format(metric_name, protected_feature))\n    for grouping_i, grouping_name_i in enumerate(bin_names):\n        j_offset = grouping_i + 1\n        for grouping_j, grouping_name_j in enumerate(bin_names[j_offset:]):\n            grouping_j += j_offset\n            is_not = ' >not<' if metric_matrix[grouping_i, grouping_j] else ''\n\n            print('    * The fairness metric is{} satisfied for \"{}\" and \"{}\" '\n                  'sub-populations.'.format(is_not, grouping_name_i,\n                                            grouping_name_j))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Equal Accuracy\n--------------\n\nFirst, let's measure whether the model is fair according to the Equal\nAccuracy metric.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get the Equal Accuracy binary matrix\nequal_accuracy_matrix = fatf_mfm.equal_accuracy(confusion_matrix_per_bin)\n\n# Print out fairness\nprint_fairness('Equal Accuracy', equal_accuracy_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Equal Opportunity\n-----------------\n\nNow, let's see whether the model is fair according to the Equal Opportunity\nmetric.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get the Equal Opportunity binary matrix\nequal_opportunity_matrix = fatf_mfm.equal_opportunity(confusion_matrix_per_bin)\n\n# Print out fairness\nprint_fairness('Equal Opportunity', equal_opportunity_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Demographic Parity\n------------------\n\nFinally, let's measure the Demographic Parity of the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get the Demographic Parity binary matrix\ndemographic_parity_matrix = fatf_mfm.demographic_parity(\n    confusion_matrix_per_bin)\n\n# Print out fairness\nprint_fairness('Demographic Parity', demographic_parity_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----\n\nBased on these results we can easily see that **Demographic Parity** is the\nonly fairness metric that is violated.\n\n----\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}