{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Measuring Robustness of a Predictive Model -- Systematic Performance Bias\n\n\nThis example illustrates how to measure Systematic Performance Bias based on a\nselected predictive performance metric (*accuracy* in this example).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Kacper Sokol <k.sokol@bristol.ac.uk>\n# License: new BSD\n\nimport fatf.utils.data.datasets as fatf_datasets\nimport fatf.utils.models as fatf_models\n\nimport fatf.accountability.models.measures as fatf_mam\n\nimport fatf.utils.metrics.subgroup_metrics as fatf_smt\n\nprint(__doc__)\n\n# Load data\niris_data_dict = fatf_datasets.load_iris()\niris_X = iris_data_dict['data']\niris_y = iris_data_dict['target'].astype(int)\niris_feature_names = iris_data_dict['feature_names']\niris_class_names = iris_data_dict['target_names']\n\n# Train a model\nclf = fatf_models.KNN()\nclf.fit(iris_X, iris_y)\n\n# Get predictions of the model for the fairness evaluation (which is also the\n# training data in this example)\niris_pred = clf.predict(iris_X)\n\n# Select a predictive performance metric\npredictive_performance_metric = 'accuracy'\n\n# Select a feature for which the difference in performance should be measured\nselected_feature_index = 1\nselected_feature_name = iris_feature_names[selected_feature_index]\n\n# Define grouping on the selected feature\nselected_feature_grouping = [3]\n\n# Compute accuracy per group in the feature\npopulation_metrics, population_names = fatf_smt.performance_per_subgroup(\n    iris_X,\n    iris_y,\n    iris_pred,\n    selected_feature_index,\n    groupings=selected_feature_grouping,\n    metric=predictive_performance_metric)\n\n# Print out performance per grouping\nprint('The *{}* for groups defined on \"{}\" feature (feature index '\n      '{}):'.format(predictive_performance_metric, selected_feature_name,\n                    selected_feature_index))\nfor p_name, p_metric in zip(population_names, population_metrics):\n    print('    * For the population split *{}* the {} is: '\n          '{:.2f}.'.format(p_name, predictive_performance_metric, p_metric))\n\n# Evaluate Systematic Performance Bias\nbias_grid = fatf_mam.systematic_performance_bias_grid(population_metrics)\n\n# Print out Systematic Performance Bias for each grouping pair\nprint('\\nThe *{}-based* Systematic Performance Bias for *{}* feature (index '\n      '{}) grouping is:'.format(predictive_performance_metric,\n                                selected_feature_name, selected_feature_index))\nfor grouping_i, grouping_name_i in enumerate(population_names):\n    j_offset = grouping_i + 1\n    for grouping_j, grouping_name_j in enumerate(population_names[j_offset:]):\n        grouping_j += j_offset\n        is_not = '' if bias_grid[grouping_i, grouping_j] else ' no'\n\n        print('    * For \"{}\" and \"{}\" groupings there >is{}< Systematic '\n              'Performance Bias.'.format(grouping_name_i, grouping_name_j,\n                                         is_not))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}